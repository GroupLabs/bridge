# LLM References

### llama.cpp
* https://github.com/ggerganov/llama.cpp

### Hugging Face models
* https://huggingface.co/meta-llama/Llama-2-7b-chat-hf
* https://huggingface.co/meta-llama/Llama-2-13b-chat-hf
* https://huggingface.co/codellama/CodeLlama-7b-hf
* https://huggingface.co/codellama/CodeLlama-13b-hf

### Quantization
Video explaining the process and math behind
* https://www.youtube.com/watch?v=mii-xFaPCrA

### ggml quants comparison
A low perplexity indicates the probability distribution is good at predicting the sample (lower is better).  
* https://github.com/ggerganov/llama.cpp#quantization
* https://github.com/ggerganov/llama.cpp/pull/1684
* https://old.reddit.com/r/LocalLLaMA/comments/13l0j7m/a_comparative_look_at_ggml_quantization_and/
* https://old.reddit.com/r/LocalLLaMA/comments/142q5k5/updated_relative_comparison_of_ggml_quantization/


### Code Llama
* Original paper: https://huggingface.co/papers/2308.12950
* Hugging Face article: https://huggingface.co/blog/codellama
* HF playground: https://huggingface.co/spaces/codellama/codellama-playground

### Hardware
* https://github.com/mlc-ai/mlc-llm/issues/15
