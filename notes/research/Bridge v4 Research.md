> Hopefully, this note turns into a publishable paper

We are building the integrative core that has a human-level system understanding. The first objective is to prove capabilities of semantic understanding of information contained in different data formats.

> By "understanding" I only mean that it is reasonably able to transduce information from natural to machine language.

So far, there are transformer-based models that are widely known to have a strong enough understanding of natural language to (arbitrarily) pass a Turing test. We explore this in the next section.

# 1. Natural Language Understanding

[Aman's Notes](https://aman.ai/read/ ) : [Aman's NLP Notes](https://aman.ai/cs224n/) 
[CPSC 224n](https://www.youtube.com/playlist?list=PLoROMvodv4rMFqRtEuo6SGjY4XbRIVRd4)
Word2Vec
TF-IDF
## (Large) Language Models

### Transformers

https://arxiv.org/abs/2304.10557
[https://ig.ft.com/generative-ai/](https://ig.ft.com/generative-ai/) (Only works in Chrome)
https://arxiv.org/abs/2311.01906
#### Attention

Attention is all you need (https://arxiv.org/abs/1706.03762)




## Optimizations
[https://x.com/amanrsanger/status/1728502445184274710?s=46&t=C-lAPJA8zJecpJNV1n8ILg](https://x.com/amanrsanger/status/1728502445184274710?s=46&t=C-lAPJA8zJecpJNV1n8ILg)
https://twitter.com/finbarrtimbers/status/1727741345983574439?s=46&t=C-lAPJA8zJecpJNV1n8ILg
https://twitter.com/marktenenholtz/status/1730987646929776940?s=46&t=C-lAPJA8zJecpJNV1n8ILg
# 2. Dependency Resolution