{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ma61yQ3kSTlz",
        "outputId": "d5cc3b18-2644-4281-d566-015670af721c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m71.7/76.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.2.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.28.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import math\n",
        "import pandas as pd\n",
        "import psycopg2\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import openai\n",
        "from io import StringIO, BytesIO\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "g5U-s6IdJFqz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mock data and models:"
      ],
      "metadata": {
        "id": "xEgH0z10SjPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MockModelRepo:\n",
        "    def __init__(self):\n",
        "        self.models = [\n",
        "            {\"id\": 1, \"name\": \"Linear Regression 1\", \"description\": \"A model suitable for text data.\"},\n",
        "            {\"id\": 2, \"name\": \"LInear Regression 2\", \"description\": \"Optimized for image recognition tasks.\"},\n",
        "        ]\n",
        "\n",
        "    def get_all_model_metadata(self):\n",
        "        return self.models"
      ],
      "metadata": {
        "id": "IWnJw0DSSpFj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model_repo.py file:"
      ],
      "metadata": {
        "id": "_r5vyXerShjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"\" #modify this to get the actual query from the user\n",
        "repsonse = \"\"\n",
        "llm_api_key = \"sk-proj-8W1tHA2pEDS5VTVw0rDHT3BlbkFJN0mGTTl6JbkhUqBJ81Wo\" # API key from OPENAI\n",
        "\n",
        "#function to calculate the confidence of gpt based on the logprobabilities of all tokens\n",
        "def calculate_confidence(logprobs):\n",
        "    # Calculate probabilities from log probabilities and average them\n",
        "    probabilities = [math.exp(logprob['logprob']) for logprob in logprobs]\n",
        "    average_probability = sum(probabilities) / len(probabilities)\n",
        "    # Convert to a percentage confidence score\n",
        "    confidence_score = average_probability * 100\n",
        "    return confidence_score\n",
        "\n",
        "# Function to determine if RAG is needed or the LLM can answer the question as is\n",
        "def is_rag_needed(query, llm_api_key):\n",
        "    # Set up the API connection\n",
        "    openai.api_key = llm_api_key\n",
        "\n",
        "    # Call the LLM with the user's query\n",
        "    try:\n",
        "        # Make a request to OpenAI's GPT-4 with log probabilities enabled\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4\",  # Specify the GPT-4 model\n",
        "            messages=[{\"role\": \"user\", \"content\": query}],\n",
        "            max_tokens=150,\n",
        "            n=1,\n",
        "            stop=None,\n",
        "            temperature=0.7,\n",
        "            logprobs=True  # Enable log probabilities for each token\n",
        "        )\n",
        "        # Analyze the LLM's response\n",
        "        # The confidence score is how confident the LLM is that it can answer the query without RAG\n",
        "        # Here we use the sum of the top log probabilities as a simple confidence measure\n",
        "        logprobs_data = response['choices'][0]['logprobs']['content']\n",
        "        # Calculate confidence score\n",
        "        confidence_score = calculate_confidence(logprobs_data)\n",
        "        print(confidence_score)\n",
        "\n",
        "        # Define a confidence threshold to decide if RAG is needed\n",
        "        confidence_threshold = 80  # will need to test with and modify this\n",
        "\n",
        "        # If the confidence score is below the threshold, RAG is needed\n",
        "        if confidence_score < confidence_threshold:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}  # Return an error as a dictionary\n",
        "\n",
        "\n",
        "\n",
        "#gets the descriptions of models in the model repo\n",
        "def fetch_model_descriptions(model_repo):\n",
        "    \"\"\"\n",
        "    Fetch metadata for all models in the repository.\n",
        "    Assume that a function to retrieve model descriptions from the repo exists.\n",
        "    \"\"\"\n",
        "    # This should return a dictionary or list of model metadata\n",
        "    return model_repo.get_all_model_metadata()\n",
        "\n",
        "#helper function querying LLM to get best model from repo\n",
        "def query_llm_for_model_selection(query, context, model_descriptions):\n",
        "    \"\"\"\n",
        "    This function uses the LLM to decide which model to select based on the user query,\n",
        "    additional context, and descriptions of available models.\n",
        "    \"\"\"\n",
        "    prompt = f\"Find the best model from the model repo based on the following: Query: {query}\\nContext: {context}\\nModel Descriptions: {model_descriptions}\\nSelect the best model:\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4\",  # Specify the GPT-4 model\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=1024,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    # Extract the suggested model from the LLM's response\n",
        "    #suggested_model = response.choices[0].text.strip()\n",
        "    suggested_model = response['choices'][0]['message']['content'].strip()\n",
        "\n",
        "    return suggested_model\n",
        "\n",
        "#function querying LLM to get best model from repo\n",
        "def select_model_for_query(query, model_repo):\n",
        "    \"\"\"\n",
        "    Main function to select the best model for a given user query.\n",
        "    \"\"\"\n",
        "    # Step 1: Fetch all the data related to the query (context)\n",
        "    context = \"a list of descriptions of documents from elasticsearch\"  # This should contain the description of text chunks from elasticsearch to pass as context\n",
        "\n",
        "    # Step 2: Fetch model descriptions\n",
        "    model_descriptions = fetch_model_descriptions(model_repo)\n",
        "\n",
        "    # Step 3: Use the LLM to suggest the best model\n",
        "    best_model_response = query_llm_for_model_selection(query, context, model_descriptions)\n",
        "\n",
        "    #need to change this to return the chosen model object not a string\n",
        "\n",
        "    return best_model_response\n",
        "\n",
        "\n",
        "\n",
        "#once data is retrieved from structured databases and elasticsearch, transform into a common type:\n",
        "def process_data(data, data_format, model_metadata, api_key):\n",
        "    # Read the data based on format\n",
        "    if data_format == 'csv':\n",
        "        df = pd.read_csv(StringIO(data))\n",
        "    elif data_format == 'xlsx' or data_format == 'xls':\n",
        "        df = pd.read_excel(BytesIO(data))\n",
        "    elif data_format == 'sql':\n",
        "        # Assuming 'data' is a SQL query already executed and returned as a string\n",
        "        df = pd.read_sql(data)\n",
        "    elif data_format == 'parquet':\n",
        "        df = pd.read_parquet(BytesIO(data))\n",
        "    elif data_format == 'json':\n",
        "        df = pd.read_json(StringIO(data))\n",
        "    elif data_format == 'xml':\n",
        "        df = pd.read_xml(StringIO(data))\n",
        "\n",
        "    # Use LLM to identify columns relevant to the model's inputs\n",
        "    openai.api_key = api_key\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"gpt-4\",\n",
        "        prompt=f\"Identify columns and data transformation needed for model with metadata: {model_metadata} in data: {df.head().to_json()}\",\n",
        "        max_tokens=150\n",
        "    )\n",
        "    preprocessing_instructions = response.choices[0].text.strip()\n",
        "\n",
        "    # Apply transformations based on LLM output or rule-based programming\n",
        "    # Example: Transforming data as per identified instructions\n",
        "    #The important columns can be stored in variables, then using a set of predefined functions we can columsn to proper output\n",
        "    #Or we can make the LLM do it all (need to experiment on what causes more time, latency...)\n",
        "    exec(preprocessing_instructions, {'df': df, 'pd': pd})  # Caution: Using exec() safely requires careful handling.\n",
        "\n",
        "    return df\n",
        "\n",
        "# Pre-trained model (hypothetically loaded)\n",
        "def predict(model, data):\n",
        "    # This function would contain code to make predictions using the loaded model\n",
        "    return model.predict(data)\n",
        "\n",
        "def main(api_key, model, data, data_format, model_metadata):\n",
        "    # Process the data\n",
        "    processed_data = process_data(data, data_format, model_metadata, api_key)\n",
        "\n",
        "    # Assuming 'model' is already trained and loaded elsewhere in your application\n",
        "    predictions = predict(model, processed_data)\n",
        "\n",
        "    # Output or use the predictions\n",
        "    print(\"Predictions:\", predictions)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EO7bu9LlR4KF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "query = \"how many burgers can i eat in an hour\"\n",
        "llm_api_key = \"sk-proj-8W1tHA2pEDS5VTVw0rDHT3BlbkFJN0mGTTl6JbkhUqBJ81Wo\" #\n",
        "model_repo = MockModelRepo  # replace with our actual model repo interface, this will likely link to a small database of models and their descriptions\n",
        "needs_rag = is_rag_needed(query, llm_api_key)\n",
        "data = \"/content/Height vs Age.xlsx\"  # This would be actual data or a path\n",
        "data_format = 'xlsx'  # For demonstration, let's say the data is in xlsx format\n",
        "print(\"RAG Needed?:\", needs_rag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yMVuFdsnrgY",
        "outputId": "8db34b70-7ce9-4b96-f9cf-0c25055c94b8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64.04097534221181\n",
            "RAG Needed?: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MockModelRepo:\n",
        "    def __init__(self):\n",
        "        self.models = [\n",
        "            {\"id\": 1, \"name\": \"Linear Regression 1\", \"description\": \"A model suitable for text data.\"},\n",
        "            {\"id\": 2, \"name\": \"LInear Regression 2\", \"description\": \"Optimized for image recognition tasks.\"},\n",
        "        ]\n",
        "\n",
        "    def get_all_model_metadata(self):\n",
        "        return self.models\n",
        "    def get_model_metadata(self, model_id):\n",
        "        return self.models.get(model_id, {})\n",
        "\n",
        "def fetch_model_descriptions(model_repo):\n",
        "    return model_repo.get_all_model_metadata()\n",
        "\n",
        "# Create an instance of MockModelRepo\n",
        "model_repo_instance = MockModelRepo()\n",
        "\n",
        "# Use the instance in your function call\n",
        "best_model = select_model_for_query(query, model_repo_instance)\n",
        "print(\"Selected Model:\", best_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4t4678W6n9I",
        "outputId": "f0fcac2b-c50e-45d9-d0f1-523585c9752a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected Model: Based on the provided descriptions, the best model for this task would be 'Linear Regression 1' as it is suitable for text data. The task involves processing a text query and context, not image recognition.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_repo = MockModelRepo()\n",
        "model_metadata = model_repo.get_model_metadata(best_model.id)\n",
        "main(llm_api_key, best_model, data, data_format, model_metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "x-Xhfow2NoyS",
        "outputId": "13734fa4-5804-416c-fd3a-5ad7b1c726a9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'str' object has no attribute 'id'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-1b9a4e73eaf8>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_repo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMockModelRepo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_repo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm_api_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'id'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usage: (needs to fix this part)"
      ],
      "metadata": {
        "id": "NHsWkb6-GJFb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SJosFArkSPIc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}