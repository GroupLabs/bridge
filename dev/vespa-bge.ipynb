{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noelthomas/Documents/GitHub/Bridge/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 22 files:   0%|          | 0/22 [00:00<?, ?it/s]\n",
      "config.json: 100%|██████████| 687/687 [00:00<00:00, 254kB/s]\n",
      "1_Pooling/config.json: 100%|██████████| 191/191 [00:00<00:00, 69.2kB/s]\n",
      "\n",
      ".gitattributes: 100%|██████████| 1.57k/1.57k [00:00<00:00, 210kB/s]\n",
      "imgs/.DS_Store: 100%|██████████| 6.15k/6.15k [00:00<00:00, 1.93MB/s]\n",
      "Fetching 22 files:   5%|▍         | 1/22 [00:00<00:06,  3.39it/s]\n",
      "config_sentence_transformers.json: 100%|██████████| 123/123 [00:00<00:00, 40.6kB/s]\n",
      "README.md: 100%|██████████| 14.1k/14.1k [00:00<00:00, 14.0MB/s]\n",
      "\n",
      "imgs/bm25.jpg: 100%|██████████| 69.0k/69.0k [00:00<00:00, 1.36MB/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "modules.json: 100%|██████████| 349/349 [00:00<00:00, 2.77MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "imgs/nqa.jpg: 100%|██████████| 158k/158k [00:00<00:00, 1.94MB/s]\n",
      "imgs/long.jpg: 100%|██████████| 485k/485k [00:00<00:00, 5.02MB/s]\n",
      "imgs/miracl.jpg: 100%|██████████| 448k/448k [00:00<00:00, 5.41MB/s]\n",
      "\n",
      "long.jpg: 100%|██████████| 127k/127k [00:00<00:00, 1.86MB/s]\n",
      "\n",
      "\n",
      "\n",
      "imgs/mkqa.jpg: 100%|██████████| 608k/608k [00:00<00:00, 4.17MB/s]\n",
      "colbert_linear.pt: 100%|██████████| 2.10M/2.10M [00:00<00:00, 13.2MB/s]\n",
      "sentence_bert_config.json: 100%|██████████| 54.0/54.0 [00:00<00:00, 519kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 964/964 [00:00<00:00, 7.14MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "sparse_linear.pt: 100%|██████████| 3.52k/3.52k [00:00<00:00, 39.0MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tokenizer_config.json: 100%|██████████| 1.31k/1.31k [00:00<00:00, 10.8MB/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "tokenizer.json: 100%|██████████| 17.1M/17.1M [00:02<00:00, 7.49MB/s]\n",
      "sentencepiece.bpe.model: 100%|██████████| 5.07M/5.07M [00:02<00:00, 2.15MB/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model.safetensors: 100%|██████████| 2.27G/2.27G [01:45<00:00, 21.6MB/s]\n",
      "pytorch_model.bin: 100%|██████████| 2.27G/2.27G [01:50<00:00, 20.6MB/s]\n",
      "Fetching 22 files: 100%|██████████| 22/22 [01:51<00:00,  5.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading existing colbert_linear and sparse_linear---------\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "passage = [\"BGE M3 is an embedding model supporting dense retrieval, lexical matching and multi-vector interaction.\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding:   0%|          | 0/1 [00:00<?, ?it/s]You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "encoding: 100%|██████████| 1/1 [00:17<00:00, 17.89s/it]\n"
     ]
    }
   ],
   "source": [
    "passage_embeddings = model.encode(passage, return_dense=True, return_sparse=True, return_colbert_vecs=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dense_vecs', 'lexical_weights', 'colbert_vecs'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passage_embeddings.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.package import Schema, Document, Field, FieldSet\n",
    "m_schema = Schema(\n",
    "            name=\"m\",\n",
    "            document=Document(\n",
    "                fields=[\n",
    "                    Field(name=\"id\", type=\"string\", indexing=[\"summary\"]),\n",
    "                    Field(name=\"text\", type=\"string\", indexing=[\"summary\", \"index\"], index=\"enable-bm25\"),\n",
    "                    Field(name=\"lexical_rep\", type=\"tensor<bfloat16>(t{})\", indexing=[\"summary\", \"attribute\"]),\n",
    "                    Field(name=\"dense_rep\", type=\"tensor<bfloat16>(x[1024])\", indexing=[\"summary\", \"attribute\"], attribute=[\"distance-metric: angular\"]),\n",
    "                    Field(name=\"colbert_rep\", type=\"tensor<bfloat16>(t{}, x[1024])\", indexing=[\"summary\", \"attribute\"])\n",
    "                ],\n",
    "            ),\n",
    "            fieldsets=[\n",
    "                FieldSet(name = \"default\", fields = [\"text\"])\n",
    "            ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.package import ApplicationPackage\n",
    "\n",
    "vespa_app_name = \"mtest\"\n",
    "vespa_application_package = ApplicationPackage(\n",
    "        name=vespa_app_name,\n",
    "        schema=[m_schema]\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.package import RankProfile, Function,  FirstPhaseRanking\n",
    "\n",
    "\n",
    "semantic = RankProfile(\n",
    "    name=\"m3hybrid\", \n",
    "    inputs=[\n",
    "        (\"query(q_dense)\", \"tensor<bfloat16>(x[1024])\"), \n",
    "        (\"query(q_lexical)\", \"tensor<bfloat16>(t{})\"), \n",
    "        (\"query(q_colbert)\", \"tensor<bfloat16>(qt{}, x[1024])\"),\n",
    "        (\"query(q_len_colbert)\", \"float\"),\n",
    "    ],\n",
    "    functions=[\n",
    "        Function(\n",
    "            name=\"dense\",\n",
    "            expression=\"cosine_similarity(query(q_dense), attribute(dense_rep),x)\"\n",
    "        ),\n",
    "        Function(\n",
    "            name=\"lexical\",\n",
    "            expression=\"sum(query(q_lexical) * attribute(lexical_rep))\"\n",
    "        ),\n",
    "        Function(\n",
    "            name=\"max_sim\",\n",
    "            expression=\"sum(reduce(sum(query(q_colbert) * attribute(colbert_rep) , x),max, t),qt)/query(q_len_colbert)\"\n",
    "        )\n",
    "    ],\n",
    "    first_phase=FirstPhaseRanking(\n",
    "        expression=\"0.4*dense + 0.2*lexical +  0.4*max_sim\",\n",
    "        rank_score_drop_limit=0.0\n",
    "    ),\n",
    "    match_features=[\"dense\", \"lexical\", \"max_sim\", \"bm25(text)\"]\n",
    ")\n",
    "m_schema.add_rank_profile(semantic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for configuration server, 0/300 seconds...\n",
      "Waiting for configuration server, 5/300 seconds...\n",
      "Using plain http against endpoint http://localhost:8080/ApplicationStatus\n",
      "Waiting for application status, 0/300 seconds...\n",
      "Using plain http against endpoint http://localhost:8080/ApplicationStatus\n",
      "Waiting for application status, 5/300 seconds...\n",
      "Using plain http against endpoint http://localhost:8080/ApplicationStatus\n",
      "Waiting for application status, 10/300 seconds...\n",
      "Using plain http against endpoint http://localhost:8080/ApplicationStatus\n",
      "Waiting for application status, 15/300 seconds...\n",
      "Using plain http against endpoint http://localhost:8080/ApplicationStatus\n",
      "Waiting for application status, 20/300 seconds...\n",
      "Using plain http against endpoint http://localhost:8080/ApplicationStatus\n",
      "Waiting for application status, 25/300 seconds...\n",
      "Using plain http against endpoint http://localhost:8080/ApplicationStatus\n",
      "Application is up!\n",
      "Finished deployment.\n"
     ]
    }
   ],
   "source": [
    "from vespa.deployment import VespaDocker\n",
    "\n",
    "vespa_docker = VespaDocker()\n",
    "app = vespa_docker.deploy(application_package=vespa_application_package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vespa_fields = {\n",
    "    \"text\": passage[0],\n",
    "    \"lexical_rep\": {key: float(value) for key, value in passage_embeddings['lexical_weights'][0].items()},\n",
    "    \"dense_rep\":passage_embeddings['dense_vecs'][0].tolist(),\n",
    "    \"colbert_rep\":  {index: passage_embeddings['colbert_vecs'][0][index].tolist() for index in range(passage_embeddings['colbert_vecs'][0].shape[0])}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.io import VespaResponse\n",
    "response: VespaResponse = app.feed_data_point(schema='m', data_id=0, fields=vespa_fields)\n",
    "assert(response.is_successful())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding:   0%|          | 0/1 [00:00<?, ?it/s]You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "encoding: 100%|██████████| 1/1 [00:14<00:00, 14.47s/it]\n"
     ]
    }
   ],
   "source": [
    "query  = [\"Why is the sky blue?\"]\n",
    "query_embeddings = model.encode(query, return_dense=True, return_sparse=True, return_colbert_vecs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_length = query_embeddings['colbert_vecs'][0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m query_fields \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput.query(q_lexical)\u001b[39m\u001b[38;5;124m\"\u001b[39m: {key: \u001b[38;5;28mfloat\u001b[39m(value) \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[43mquery_embeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlexical_weights\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mitems()},\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput.query(q_dense)\u001b[39m\u001b[38;5;124m\"\u001b[39m: query_embeddings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdense_vecs\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput.query(q_colbert)\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;28mstr\u001b[39m({index: query_embeddings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolbert_vecs\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][index]\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(query_embeddings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolbert_vecs\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])}),\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput.query(q_len_colbert)\u001b[39m\u001b[38;5;124m\"\u001b[39m: query_length\n\u001b[1;32m      6\u001b[0m }\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "query_fields = {\n",
    "    \"input.query(q_lexical)\": {key: float(value) for key, value in query_embeddings['lexical_weights'][0].items()},\n",
    "    \"input.query(q_dense)\": query_embeddings['dense_vecs'][0].tolist(),\n",
    "    \"input.query(q_colbert)\":  str({index: query_embeddings['colbert_vecs'][0][index].tolist() for index in range(query_embeddings['colbert_vecs'][0].shape[0])}),\n",
    "    \"input.query(q_len_colbert)\": query_length\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"index:mtest_content/0/cfcd2084234135f700f08abf\",\n",
      "  \"relevance\": 0.24681896577832974,\n",
      "  \"source\": \"mtest_content\",\n",
      "  \"fields\": {\n",
      "    \"matchfeatures\": {\n",
      "      \"bm25(text)\": 0.28768207245178085,\n",
      "      \"dense\": 0.2560008149555224,\n",
      "      \"lexical\": 0.017232894897460938,\n",
      "      \"max_sim\": 0.35243015204157147\n",
      "    },\n",
      "    \"text\": \"BGE M3 is an embedding model supporting dense retrieval, lexical matching and multi-vector interaction.\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from vespa.io import VespaQueryResponse\n",
    "import json\n",
    "\n",
    "response:VespaQueryResponse = app.query(\n",
    "    yql=\"select id, text from m where userQuery() or ({targetHits:10}nearestNeighbor(dense_rep,q_dense))\",\n",
    "    ranking=\"m3hybrid\",\n",
    "    query=query[0],\n",
    "    body={\n",
    "        **query_fields\n",
    "    }\n",
    ")\n",
    "assert(response.is_successful())\n",
    "print(json.dumps(response.hits[0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0173909030854702"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compute_lexical_matching_score(passage_embeddings['lexical_weights'][0], query_embeddings['lexical_weights'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25596598"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embeddings['dense_vecs'][0] @ passage_embeddings['dense_vecs'][0].T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3544)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.colbert_score(query_embeddings['colbert_vecs'][0],passage_embeddings['colbert_vecs'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
